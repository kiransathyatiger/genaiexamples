{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkn8C0ik96kc",
        "outputId": "60fa375c-28e5-4140-8f9e-0642ffe806ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (23.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8xhdQu62qgR",
        "outputId": "3c179702-6d92-4f91-d118-a1166596668a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: SentencePiece in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk2u-sVeA_ps",
        "outputId": "4a3dcddd-5d2b-40a3-efb4-e79a28f0f6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAc33SoE_vCx",
        "outputId": "19cd0154-762f-42a7-8a0a-46e4fe6aaa07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting transformers==4.12.0\n",
            "  Using cached transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
            "Requirement already satisfied: filelock in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (2023.8.8)\n",
            "Requirement already satisfied: requests in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.12.0)\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.0)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.12.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.0.17->transformers==4.12.0) (2023.9.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.0.17->transformers==4.12.0) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers==4.12.0) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.12.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.12.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.12.0) (2023.7.22)\n",
            "Requirement already satisfied: six in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from sacremoses->transformers==4.12.0) (1.16.0)\n",
            "Requirement already satisfied: click in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from sacremoses->transformers==4.12.0) (8.1.6)\n",
            "Requirement already satisfied: joblib in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from sacremoses->transformers==4.12.0) (1.3.2)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  Building wheel for tokenizers (pyproject.toml): started\n",
            "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
            "Failed to build tokenizers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [51 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-311\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
            "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
            "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
            "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
            "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
            "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
            "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
            "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
            "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
            "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
            "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
            "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
            "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
            "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
            "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
            "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
            "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
            "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
            "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
            "      running build_ext\n",
            "      running build_rust\n",
            "      error: can't find Rust compiler\n",
            "      \n",
            "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
            "      \n",
            "      To update pip, run:\n",
            "      \n",
            "          pip install --upgrade pip\n",
            "      \n",
            "      and then retry package installation.\n",
            "      \n",
            "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for tokenizers\n",
            "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyPDF2 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.12.0\n",
        "!pip install pyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_iy408VxFl0",
        "outputId": "df101630-ed9a-442e-b6b5-b5570f914ae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pytesseract in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\kiran.sathyanara\\appdata\\roaming\\python\\python311\\site-packages (from pytesseract) (10.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GtX6aesLA3-0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Directory for storing PDF resumes and job applications\n",
        "pdf_directory = r'C:\\Users\\kiran.sathyanara\\Downloads\\content\\pdf_files'\n",
        "\n",
        "# Directory for storing extracted text from PDFs\n",
        "text_directory = r'C:\\Users\\kiran.sathyanara\\Downloads\\content\\extracted_text'\n",
        "\n",
        "# OCR output directory for scanned PDFs\n",
        "ocr_directory = r'C:\\Users\\kiran.sathyanara\\Downloads\\content\\ocr_output'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(pdf_directory, exist_ok=True)\n",
        "os.makedirs(text_directory, exist_ok=True)\n",
        "os.makedirs(ocr_directory, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PXdkESu141sf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kiran S resume.pdf.pdf\n",
            "[{'/Type': '/Page', '/Parent': IndirectObject(2, 0, 2977618098512), '/Resources': {'/Font': {'/F1': IndirectObject(5, 0, 2977618098512), '/F2': IndirectObject(9, 0, 2977618098512), '/F3': IndirectObject(12, 0, 2977618098512), '/F4': IndirectObject(17, 0, 2977618098512), '/F5': IndirectObject(19, 0, 2977618098512), '/F6': IndirectObject(24, 0, 2977618098512), '/F7': IndirectObject(29, 0, 2977618098512), '/F8': IndirectObject(34, 0, 2977618098512)}, '/ExtGState': {'/GS7': IndirectObject(7, 0, 2977618098512), '/GS8': IndirectObject(8, 0, 2977618098512)}, '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']}, '/Annots': [IndirectObject(11, 0, 2977618098512)], '/MediaBox': [0, 0, 594.96, 842.04], '/Contents': IndirectObject(4, 0, 2977618098512), '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'}, '/Tabs': '/S', '/StructParents': 0}]\n",
            "[{'/Type': '/Page', '/Parent': IndirectObject(2, 0, 2977618098512), '/Resources': {'/Font': {'/F1': IndirectObject(5, 0, 2977618098512), '/F8': IndirectObject(34, 0, 2977618098512), '/F3': IndirectObject(12, 0, 2977618098512), '/F4': IndirectObject(17, 0, 2977618098512), '/F6': IndirectObject(24, 0, 2977618098512), '/F2': IndirectObject(9, 0, 2977618098512), '/F7': IndirectObject(29, 0, 2977618098512), '/F9': IndirectObject(41, 0, 2977618098512)}, '/ExtGState': {'/GS7': IndirectObject(7, 0, 2977618098512), '/GS8': IndirectObject(8, 0, 2977618098512)}, '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']}, '/MediaBox': [0, 0, 594.96, 842.04], '/Contents': IndirectObject(40, 0, 2977618098512), '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'}, '/Tabs': '/S', '/StructParents': 2}]\n",
            "[{'/Type': '/Page', '/Parent': IndirectObject(2, 0, 2977618098512), '/Resources': {'/Font': {'/F1': IndirectObject(5, 0, 2977618098512), '/F6': IndirectObject(24, 0, 2977618098512), '/F2': IndirectObject(9, 0, 2977618098512), '/F3': IndirectObject(12, 0, 2977618098512), '/F4': IndirectObject(17, 0, 2977618098512), '/F7': IndirectObject(29, 0, 2977618098512), '/F8': IndirectObject(34, 0, 2977618098512)}, '/ExtGState': {'/GS7': IndirectObject(7, 0, 2977618098512), '/GS8': IndirectObject(8, 0, 2977618098512)}, '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']}, '/MediaBox': [0, 0, 594.96, 842.04], '/Contents': IndirectObject(44, 0, 2977618098512), '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'}, '/Tabs': '/S', '/StructParents': 3}]\n"
          ]
        }
      ],
      "source": [
        " #Directory for storing PDF resumes and job applications\n",
        "pdf_directory = r'c:\\Users\\kiran.sathyanara\\Downloads\\content\\pdf_files'\n",
        "\n",
        "resume_files = []\n",
        "for file_name in os.listdir(pdf_directory):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        print(file_name)\n",
        "        resume_files.append(os.path.join(pdf_directory, file_name))\n",
        "\n",
        "resume_summaries = []  # To store the generated summaries\n",
        "\n",
        "# Loop through each resume file\n",
        "for resume_file in resume_files:\n",
        "    with open(resume_file, 'rb') as file:\n",
        "        # Create a PDF reader object\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Extract text from each page\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            print([page])\n",
        "            text += page.extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsgl7KLQ1hIB",
        "outputId": "e1e37a12-8651-4f21-8c22-798109d9af13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kiran.sathyanara\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "C:\\Users\\kiran.sathyanara\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary for Resume 1:\n",
            "<pad>KIRAN SATHYANARAYANA (D ATA SCIENCE) has 14 years of experience and 12 of those in data science. worked at major fortune 100 clients and product firms across the us, Singapore and india. built a customized sampling algorithm to streamline the QA process which improved the quality by 70% and re duced the manual effort of the team by 80%. mentored junior analysts and junior data scientists on entire life-cycle of\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Continuing the loop from the previous step\n",
        "from transformers  import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Encode the text\n",
        "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1000, truncation=True)\n",
        "\n",
        "# Generate the summary\n",
        "outputs = model.generate(inputs, max_length=100, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode the summary\n",
        "summary = tokenizer.decode(outputs[0])\n",
        "\n",
        "resume_summaries.append(summary)\n",
        "\n",
        "# Print the generated summaries for each resume\n",
        "for i, summary in enumerate(resume_summaries):\n",
        "    print(f\"Summary for Resume {i+1}:\")\n",
        "    print(summary)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bxuJYmv1BbJ"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fdjm-U41B4r"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9OYyb2Tc5Eis"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: \"'langchain[all]'\"\n"
          ]
        }
      ],
      "source": [
        "pip install 'langchain[all]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8e4aVeV9ydn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
